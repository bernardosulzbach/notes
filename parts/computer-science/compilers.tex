\chapter{Compilers}

\section{Syntax-directed translation}

\subsection{Abstract and concrete syntax}\label{Abstract and concrete syntax}

An \textbf{abstract syntax tree}, is a data structure in which each interior node represents an operator. The children of a node represent the operands of this operator.
The concept a syntax tree can be generalized so that any programming construct can be treated as an operator with some operands \cite[p.~69]{compilers-aho-2007}.

Abstract syntax trees differ from \textbf{concrete syntax trees}, also known as \textbf{parse trees}, because the interior nodes of a abstract syntax tree represent programming constructs while the interior nodes of a parse tree represent nonterminals of the concrete syntax \cite[p.~69]{compilers-aho-2007}.

\section{Code generation}

\subsection{Optimization of basic blocks}

\textbf{Local optimizations} are optimizations performed within a basic block.

A basic block can be represented as a directed acyclic graph (DAG).

\subsubsection{Local common subexpression elimination}

Local common subexpression elimination consists in eliminating expressions that compute a value that has already been computed \cite[p.~533]{compilers-aho-2007}.

For instance, in the following sequence of instructions, it would be interesting to compute the value of \texttt{b} and \texttt{d} only once.

\begin{lstlisting}
a = b + c
b = a - d
c = b + c
d = a - d
\end{lstlisting}

Note that in more elaborate examples, algebraic identities may need to be used before local common subexpression elimination can be applied.

\begin{lstlisting}
a = b + c
b = b - d
c = c + d
e = b + c
\end{lstlisting}

In the above example, even though \texttt{b} and \texttt{c} both change between the first and last statements, their sum remains the same.

\subsubsection{Dead code elimination}

Dead code elimination on DAG's correspond to removing roots (nodes with no ancestors) which have no live variables attached to it.

\subsubsection{Use of algebraic identities}

Algebraic identities represent another important class of optimization on basic blocks.

Algebraic identities can be used to remove \textbf{arithmetic identities} such as adding zero and multiplying by one \cite[p.~536]{compilers-aho-2007}.
\[x + 0 = 0 + x = x\]
\[x \cdot 1 = 1 \cdot x = x\]
They can also be used to perform \textbf{strength reduction}, that is, to replace more expensive operators by cheaper ones \cite[p.~536]{compilers-aho-2007}.
\[x^{2} = x \times x\]
\[x / 2 = x >> 1\]
A third class of algebraic identities is \textbf{constant folding}, in which constant expressions are evaluated at compile time \cite[p.~536]{compilers-aho-2007}.
\[2 * 32 = 64\]
Lastly, there are also more general algebraic transformations such as \textbf{commutativity and associativity} \cite[p.~536]{compilers-aho-2007}.

\begin{lstlisting}
a = b + c
t = c + d
e = t + b
\end{lstlisting}

In the listing above, if \texttt{t} is not needed afterwards, commutativity and associativity could be used to simplify the code to the following.

\begin{lstlisting}
a = b + c
e = a + d
\end{lstlisting}

It is important to determine which computation rearrangements are permitted in order to not introduce the possibility of overflow or precision loss during optimization.
For instance, the Fortran standard states that a compiler may evaluate any mathematically equivalent expression, provided that the integrity of parentheses is not violated.
Thus, a compiler may evaluate \(x * y - x * z\) as \(x * (y - z)\), but it may not evaluate \(a + (b - c)\) as \((a + b) - c\).
A Fortran compiler must therefore keep track of where parentheses were present in the source code in order to optimize programs in accordance with the language definition \cite[p.~537]{compilers-aho-2007}.

\section{Libraries}

\subsection{Global offset table}

If the dynamic linker could only perform relocations, when the value of global symbol is required, the dynamic linker would have to look up the memory address of that symbol and rewrite the code to load that address \cite{computer-science-from-the-bottom-up}.

An enhancement would be to set aside space in the binary to hold the address of that symbol, and use the dynamic linker to put the address there rather than in the code directly.
This way the code of the binary would never have to be changed in runtime.
This area exists and is called the global offset table (GOT) \cite{computer-science-from-the-bottom-up}.

\subsection{Procedure lookup table}

The procedure lookup table (PLT) serves to facilitate \textit{lazy binding} in programs.
Binding is the name given to assigning an entry of the table the real address \cite{computer-science-from-the-bottom-up}.

Binding a function involves loading library code into memory if this hasn't been done yet, searching through it to find the function being called, and writing its real address to memory.
Therefore, binding unused functions is wasteful.
Each library function has an entry in the PLT, which initially points to a dummy function \cite{computer-science-from-the-bottom-up}.

When the program calls the function, it is actually calling the PLT entry.
This function loads a few parameters that are needed by the dynamic linker for it to resolve the function and then calls into a special lookup function of the dynamic linker.
The dynamic linker then changes the stub code so that the next time the function is called it will go straight to the right address \cite{computer-science-from-the-bottom-up}.

While the PLT is usually filled lazily in runtime, the GOT is filled before the program begins executing \cite{computer-science-from-the-bottom-up}.

\section{Instruction level parallelism}

Optimized code scheduling takes advantage of modern computer architectures.
Many machines allow pipelined execution and some allow several instructions to begin at the same time.
When scheduling instructions, one must be aware of the \textbf{data dependencies} between instructions.
\textbf{The only true data dependency is a read after a write}.
True data dependencies cannot be eliminated and must be executed in order \cite[p.~765]{compilers-aho-2007}.

The data-dependence graphs for basic blocks represent timing constrains across statements.
These graphs are always acyclic \cite[p.~765]{compilers-aho-2007}.

\textbf{Interblock code motion} is the name given to the process of moving statements from one basic block into another.
It may elicit new parallelism opportunities \cite[p.~765]{compilers-aho-2007}.

\section{Solved exercises}

\subsection{Solved course exercises}

\begin{Exercise}[difficulty=1]
What is an abstract syntax tree?
\end{Exercise}
\begin{Answer}
See \ref{Abstract and concrete syntax}.
\end{Answer}

\begin{Exercise}
When is the abstract syntax tree necessary?
\end{Exercise}
\begin{Answer}
The abstract syntax tree is necessary when the language being compiled is complex and decoupling parsing and code generation is required to reduce the complexity of the compiler.

Very simple languages such as a basic calculator can be compiled without ever resorting to an abstract syntax tree.
\end{Answer}

\begin{Exercise}
What is syntax-directed translation?
\end{Exercise}
\begin{Answer}
Syntax-directed translation is the process of walking through a parse tree and generating intermediary code from it \cite[p.~12]{compilers-aho-2007}.

It is done by attaching rules or program fragments to productions in a grammar \cite[p.~52]{compilers-aho-2007}.
\end{Answer}

\begin{Exercise}
What is an annotated parse tree? What are attributes?
\end{Exercise}
\begin{Answer}
An annotated parse tree is a parse tree with attributes at each node.
Attributes are information about that node of the parse tree, usually types and values.
\end{Answer}

\begin{Exercise}
When are the parse tree attributes created?
\end{Exercise}
\begin{Answer}
The attributes are created during \textbf{syntactic analysis} \cite[p.~54]{compilers-aho-2007}.
\end{Answer}

\begin{Exercise}
In which order the attributes should be evaluated?
\end{Exercise}
\begin{Answer}
The attributes should be evaluated from the leaves to the root.
\end{Answer}

\begin{Exercise}
How are the attributes implemented in a bottom-up analyzer?
\end{Exercise}
\begin{Answer}
In a bottom-up analyzer, the attributes are implemented by first resolving the attributes of the child nodes and then synthesizing the attributes of the parents.
\end{Answer}

\begin{Exercise}
What is a translation scheme?
\end{Exercise}
\begin{Answer}
A translation scheme is a notation that embeds program fragments called
\textbf{semantic actions} in production bodies.
The actions are executed in the order that the productions are used during syntax analysis \cite[p.~107]{compilers-aho-2007}.
\end{Answer}

\begin{Exercise}
Through which steps can any translation scheme be implemented?
\end{Exercise}
\begin{Answer}
Any (syntax-directed) translation scheme can be implemented by first \textbf{building a parse tree and performing the actions in a left-to-right depth-first order}. This is also known as a a preorder traversal.
Typically, translation schemes are implemented during parsing, without building a parse tree \cite[p.~324]{compilers-aho-2007}.
\end{Answer}

\begin{Exercise}
What is an attribute grammar?
\end{Exercise}
\begin{Answer}
An attribute grammar is a syntax-directed definition without side effects.
That is, an attribute grammar has rules which define \textbf{the value of an attribute purely in terms of other attributes and constants} \cite[p.~306]{compilers-aho-2007}.
\end{Answer}

\begin{Exercise}
What is an L-attributed translation scheme?
\end{Exercise}
\begin{Answer}
L-attributed translation schemes are a special type of attribute grammars.
Their attributes can be evaluated in one left-to-right traversal of the abstract syntax tree.
As a result, \textbf{attribute evaluation can be incorporated conveniently in top-down parsing}.
Many programming languages are L-attributed.
\end{Answer}

\begin{Exercise}
What is an S-attributed translation scheme?
\end{Exercise}
\begin{Answer}
S-attributed translation schemes are a class of attribute grammars which have no inherited attributes.
Inherited attributes, which must be passed down from parent nodes to children nodes of the abstract syntax tree during the semantic analysis of the parsing process, are a problem for bottom-up parsing as the parent nodes of the abstract syntax tree are created after creation of all of their children.
Attribute evaluation in S-attributed grammars can be incorporated conveniently in both top-down parsing and bottom-up parsing.

\textbf{Any S-attributed grammar is also an L-attributed grammar}.
\end{Answer}

\begin{Exercise}
List three semantic actions typical to the creation of the abstract syntax tree in an analyzer written using Yacc.
\end{Exercise}
\begin{Answer}
\begin{enumerate}
\item Create a list of abstract syntax trees, such as the parameters of a function definition.
\item Create a abstract syntax tree leaf node for a symbol.
\item Join multiple nodes to build a larger node for a variable declaration, such as the token used to name the type and the token used for the name.
\end{enumerate}
\end{Answer}

\begin{Exercise}
Which type of translation scheme can be implemented using Yacc?
\end{Exercise}
\begin{Answer}
Yacc can be used to implement S-attributed translation schemes.
\end{Answer}

\begin{Exercise}
What is a three-address code structure? What is its purpose?
\end{Exercise}
\begin{Answer}
It is a structure which represents an operation over up to two operands and assigns the result to a third operator.
Its purpose is to serve as a intermediary representation which is closer to machine code before code generation happens \cite[p.~358]{compilers-aho-2007}.
\end{Answer}

\begin{Exercise}
Construct the three-address code structure for an if-then-else statement.
\end{Exercise}
\begin{Answer}
\begin{lstlisting}
d = 0;
if (a < b) {
    d += d;
}
e = d / 2;
\end{lstlisting}
From the above source code one can derive the following three-address code.
\begin{lstlisting}
  d = 0
  _T0 = a < b
  jump L1 if not _T0
_L0:
  d = d + 2
_L1:
  e = d / 2
\end{lstlisting}
\end{Answer}

\begin{Exercise}[difficulty=1]
How does panic mode error recovery works?
\end{Exercise}
\begin{Answer}
Panic mode error recovery works by skipping input symbols until a token in a selected set of synchronization tokens is found.
The results are as good as the synchronizing set chosen.
The synchronizing set should be so that the parser recovers quickly from the errors that are most likely to occur in the real world \cite[p.~229]{compilers-aho-2007}.
\end{Answer}

\begin{Exercise}
What is the other main error recovery technique?
\end{Exercise}
\begin{Answer}
The other main error recovery strategy is \textbf{phrase-level recovery}.
It is implemented by filling in the blank entries in the predictive parsing table with pointers to error handling functions.
These functions then change, insert, or delete symbols on the input and issue appropriate error messages \cite[p.~231]{compilers-aho-2007}.
\end{Answer}

\begin{Exercise}
Why isn't there a deterministic algorithm for error recovery?
\end{Exercise}
\begin{Answer}
In any nontrivial programming language, the compiler has no way of knowing for sure what program the programmer wanted to write.
For instance, assuming that what happened is a missing semicolon might eventually not be correct as the programmer might just have wanted to continue the statement in the next line but forgot an operator.
\end{Answer}

\begin{Exercise}
How is error correction implemented in a reductive algorithm?
\end{Exercise}

\begin{Exercise}[difficulty=1]
How is error recovery implemented in Yacc?
\end{Exercise}
\begin{Answer}
In Yacc, error recovery uses a form of error productions.
The user has to decide what non-terminals will have error recovery.
The user then adds to the grammar error productions using the \texttt{error} reserved word.
Yacc then generates a parser from this specification, treating error productions as ordinary productions.
However, when an error is encountered, the parser will ``shift'' a fictitious error token onto the stack, as if the \texttt{error} token was part of the input.
The associated semantic action will be invoked when the production is fully matched \cite[p.~295]{compilers-aho-2007}.
\end{Answer}

\begin{Exercise}
What are the error correction actions inserted in LL(1) tables of top-down analysis?
\end{Exercise}

\begin{Exercise}[difficulty=1]
List three loop optimization techniques and give small code examples.
\end{Exercise}
\begin{Answer}
\begin{enumerate}
\item \textbf{Loop unrolling} consists of replicating the loop body multiple times.
This may improve performance by \textbf{evaluating the loop condition less times and reducing branching}.
\begin{lstlisting}
for (i = 0; i < n; i++) {
    S(i);
}
\end{lstlisting}
\begin{lstlisting}
for (i = 0; i + 4 < n; i += 4) {
    S(i);
    S(i + 1);
    S(i + 2);
    S(i + 3);
}
for (; i < n; i++) {
    S(i);
}
\end{lstlisting}
\item \textbf{Loop fission} consists of breaking a loop into multiple loops over the same index range.
This may improve performance by \textbf{improving locality of reference}.
\begin{lstlisting}
for (i = 0; i < n; i++) {
    a[i] = a[i - 1] + 1;
    b[i] = b[i - 1] - 1;
}
\end{lstlisting}
\begin{lstlisting}
for (i = 0; i < n; i++) {
    a[i] = a[i - 1] + 1;
}
for (i = 0; i < n; i++) {
    b[i] = b[i - 1] - 1;
}
\end{lstlisting}
\item \textbf{Loop fusion} consists of combining the bodies of two adjacent loops that iterate the same number of times.
This may improve performance by \textbf{evaluating the loop condition less times and reducing branching}.
\begin{lstlisting}
for (i = 0; i < n; i++) {
    a[i] = a[i - 1] + 1;
}
for (i = 0; i < n; i++) {
    b[i] = b[i - 1] - 1;
}
\end{lstlisting}
\begin{lstlisting}
for (i = 0; i < n; i++) {
    a[i] = a[i - 1] + 1;
    b[i] = b[i - 1] - 1;
}
\end{lstlisting}
\end{enumerate}
\end{Answer}

\begin{Exercise}[difficulty=1]
What is a basic block and how is it used for optimization?
\end{Exercise}
\begin{Answer}
Basic blocks are sequences of instructions that are always executed one after the other, with no branching \cite[p.~92]{compilers-aho-2007}.

Basic blocks which are likely to be required at the same time can be laid out in a way so that they are close in memory, improving the spatial locality of code access \cite[p.~457]{compilers-aho-2007}.

Code improvement within a basic block can be very creative, as long as the end result of executing the entire new basic block is the same as executing the entire original basic block.
\end{Answer}

\begin{Exercise}
What is optimization by tree rewriting?
\end{Exercise}

\begin{Exercise}
When is optimization performed during the compilation process?
\end{Exercise}

\begin{Exercise}
How does expression optimization works even where there is no common sub-expressions?
\end{Exercise}

\begin{Exercise}[difficulty=1]
What is data lifetime? Why is this concept important?
\end{Exercise}
\begin{Answer}
Data lifetime indicates for how long a computed value has to be kept.
It can be thought of as \textbf{storage duration}.

The concept is important as it allows for compiler optimizations such as performing stack allocations instead of heap allocations and for the compiler to determine when it can reuse a register to store another value.

Understanding data lifetime is useful when scheduling instructions during the optimization phases because it is another factor to be considered when reordering instructions.
Having registers with unnecessarily long lifetimes is usually inefficient, so pulling the definitions closer to the uses tends to improve performance.
\end{Answer}

\begin{Exercise}
How does the left-edge register allocation algorithm works?
\end{Exercise}

\begin{Exercise}
What is the main limitation of performing left-edge register allocation?
\end{Exercise}